{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((573417, 2600), (190624, 2600))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\"\n",
    "\n",
    "import numpy as np\n",
    "tra_features = np.load('train_features.npy').astype(np.float32)\n",
    "tra_targets = np.load('train_targets.npy').astype(np.int64)\n",
    "val_features = np.load('val_features.npy').astype(np.float32)\n",
    "val_targets = np.load('val_targets.npy').astype(np.int64)\n",
    "tra_features.shape, val_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amax/miniconda3/lib/python3.7/site-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.metrics.scorer module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.metrics. Anything that cannot be imported from sklearn.metrics is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import optim\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from skorch import NeuralNetClassifier\n",
    "import sklearn\n",
    "from skorch.callbacks import EpochScoring, LRScheduler, Checkpoint\n",
    "from torch.optim import Adam, SGD\n",
    "import adamod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skorch.dataset import Dataset\n",
    "from skorch.helper import predefined_split\n",
    "valid_ds = Dataset(val_features, val_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DownSample2x(nn.Sequential):\n",
    "    def __init__(self, _in, _out):\n",
    "        super().__init__(\n",
    "            nn.Conv1d(_in, _out, kernel_size=2, stride=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "class SELayer(nn.Module):\n",
    "    def __init__(self, _in, _hidden=64):\n",
    "        super(SELayer, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(_in, _hidden),\n",
    "            nn.PReLU(),\n",
    "            nn.Linear(_hidden, _in),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _ = x.size()\n",
    "        y = self.avg_pool(x).view(b, c)\n",
    "        y = self.fc(y).view(b, c, 1)\n",
    "        return x * y\n",
    "    \n",
    "class ResConv1d(nn.Module):\n",
    "    def __init__(self, _in, _out):\n",
    "        super(ResConv1d, self).__init__()\n",
    "        \n",
    "        self.cal = nn.Sequential(\n",
    "            nn.Conv1d(_in, _out, kernel_size=3, padding=1, stride=1),\n",
    "            nn.BatchNorm1d(_out),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(_out, _out, kernel_size=3, padding=1, stride=1),\n",
    "            nn.BatchNorm1d(_out),\n",
    "        )\n",
    "        self.se = SELayer(_out, _out)\n",
    "        self.conv = nn.Conv1d(_in, _out, kernel_size=1, padding=0, stride=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.bn = nn.BatchNorm1d(_out)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        res = self.cal(x)\n",
    "        res = self.se(res)\n",
    "        \n",
    "        x = self.bn(self.conv(x))\n",
    "        \n",
    "        return self.relu(res + x)\n",
    "        \n",
    "class MyModule(nn.Module):\n",
    "    def __init__(self, nonlin=F.relu):\n",
    "        super(MyModule, self).__init__()\n",
    "        \n",
    "        self.d1 = DownSample2x(1, 64)\n",
    "        self.c1 = ResConv1d(64, 64)\n",
    "        \n",
    "        self.d2 = DownSample2x(64, 64)\n",
    "        self.c2 = ResConv1d(64, 64)\n",
    "        \n",
    "        self.d3 = DownSample2x(64, 64)\n",
    "        self.c3 = ResConv1d(64, 64)\n",
    "        \n",
    "        self.d4 = DownSample2x(64, 64)\n",
    "        self.c4 = ResConv1d(64, 64)\n",
    "        \n",
    "        self.d5 = DownSample2x(64, 64)\n",
    "        self.c5 = ResConv1d(64, 64)\n",
    "        \n",
    "        self.d6 = DownSample2x(64, 64)\n",
    "        self.c6 = ResConv1d(64, 64)\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.cls = nn.Linear(64 * 40, 3)\n",
    "        \n",
    "    def preprocess(self, x, p=2, eps=1e-8):\n",
    "        x = x / (x.norm(p=p, dim=1, keepdim=True)+eps)\n",
    "        x = x.unsqueeze(1)\n",
    "        return x\n",
    "        \n",
    "    def forward(self, x):\n",
    "        bs = x.size(0)\n",
    "        x = self.preprocess(x)\n",
    "        \n",
    "        x = self.d1(x)\n",
    "        x = self.c1(x)\n",
    "        \n",
    "        x = self.d2(x)\n",
    "        x = self.c2(x)\n",
    "        \n",
    "        x = self.d3(x)\n",
    "        x = self.c3(x)\n",
    "\n",
    "        x = self.d4(x)\n",
    "        x = self.c4(x)\n",
    "        \n",
    "        x = self.d5(x)\n",
    "        x = self.c5(x)\n",
    "        \n",
    "        x = self.d6(x)\n",
    "        x = self.c6(x)\n",
    "        \n",
    "        x = x.reshape(bs, -1)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        return F.softmax(self.cls(x))\n",
    "\n",
    "    \n",
    "def microf1(net, ds, y=None):\n",
    "    y_true = [y for _, y in ds]\n",
    "    y_pred = net.predict(ds)\n",
    "    return sklearn.metrics.f1_score(y_true, y_pred,average='micro')\n",
    "def macrof1(net, ds, y=None):\n",
    "    y_true = [y for _, y in ds]\n",
    "    y_pred = net.predict(ds)\n",
    "    return sklearn.metrics.f1_score(y_true, y_pred,average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amax/miniconda3/lib/python3.7/site-packages/ipykernel_launcher.py:104: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    macrof1    microf1    train_loss    valid_acc    valid_loss    cp       dur\n",
      "-------  ---------  ---------  ------------  -----------  ------------  ----  --------\n",
      "      1     \u001b[36m0.9740\u001b[0m     \u001b[32m0.9929\u001b[0m        \u001b[35m0.0390\u001b[0m       \u001b[31m0.9929\u001b[0m        \u001b[94m0.0231\u001b[0m     +  225.8468\n",
      "      2     \u001b[36m0.9777\u001b[0m     \u001b[32m0.9943\u001b[0m        \u001b[35m0.0209\u001b[0m       \u001b[31m0.9943\u001b[0m        \u001b[94m0.0188\u001b[0m     +  225.5386\n",
      "      3     0.9368     0.9834        \u001b[35m0.0183\u001b[0m       0.9834        0.0530        225.2604\n",
      "      4     \u001b[36m0.9806\u001b[0m     \u001b[32m0.9953\u001b[0m        \u001b[35m0.0167\u001b[0m       \u001b[31m0.9953\u001b[0m        \u001b[94m0.0160\u001b[0m     +  226.3929\n"
     ]
    }
   ],
   "source": [
    "net = NeuralNetClassifier(\n",
    "    MyModule,\n",
    "    max_epochs=20,\n",
    "    lr=0.001,\n",
    "    batch_size=256,\n",
    "    optimizer=Adam,\n",
    "    iterator_train__shuffle=True,\n",
    "    train_split=predefined_split(valid_ds),\n",
    "    callbacks=[EpochScoring(macrof1, use_caching=True, lower_is_better=False),\n",
    "               EpochScoring(microf1, use_caching=True, lower_is_better=False),\n",
    "               Checkpoint(monitor='macrof1_best', dirname='02exp')],\n",
    "    device='cuda'\n",
    ")\n",
    "\n",
    "net.fit(tra_features, tra_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MacroLoss Fintuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amax/miniconda3/lib/python3.7/site-packages/ipykernel_launcher.py:104: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    macrof1    microf1    train_loss    valid_acc    valid_loss    cp       dur\n",
      "-------  ---------  ---------  ------------  -----------  ------------  ----  --------\n",
      "      1     \u001b[36m0.9855\u001b[0m     \u001b[32m0.9966\u001b[0m        \u001b[35m0.3012\u001b[0m       \u001b[31m0.9966\u001b[0m        \u001b[94m0.3042\u001b[0m     +  238.4180\n",
      "      2     0.9855     0.9965        \u001b[35m0.2981\u001b[0m       0.9965        \u001b[94m0.3040\u001b[0m        239.8339\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<class 'skorch.classifier.NeuralNetClassifier'>[initialized](\n",
       "  module_=MyModule(\n",
       "    (d1): DownSample2x(\n",
       "      (0): Conv1d(1, 64, kernel_size=(2,), stride=(2,))\n",
       "      (1): ReLU(inplace=True)\n",
       "    )\n",
       "    (c1): ResConv1d(\n",
       "      (cal): Sequential(\n",
       "        (0): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "        (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "        (3): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "        (4): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (se): SELayer(\n",
       "        (avg_pool): AdaptiveAvgPool1d(output_size=1)\n",
       "        (fc): Sequential(\n",
       "          (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (1): PReLU(num_parameters=1)\n",
       "          (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (3): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "      (conv): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
       "      (relu): ReLU()\n",
       "      (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (d2): DownSample2x(\n",
       "      (0): Conv1d(64, 64, kernel_size=(2,), stride=(2,))\n",
       "      (1): ReLU(inplace=True)\n",
       "    )\n",
       "    (c2): ResConv1d(\n",
       "      (cal): Sequential(\n",
       "        (0): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "        (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "        (3): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "        (4): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (se): SELayer(\n",
       "        (avg_pool): AdaptiveAvgPool1d(output_size=1)\n",
       "        (fc): Sequential(\n",
       "          (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (1): PReLU(num_parameters=1)\n",
       "          (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (3): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "      (conv): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
       "      (relu): ReLU()\n",
       "      (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (d3): DownSample2x(\n",
       "      (0): Conv1d(64, 64, kernel_size=(2,), stride=(2,))\n",
       "      (1): ReLU(inplace=True)\n",
       "    )\n",
       "    (c3): ResConv1d(\n",
       "      (cal): Sequential(\n",
       "        (0): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "        (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "        (3): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "        (4): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (se): SELayer(\n",
       "        (avg_pool): AdaptiveAvgPool1d(output_size=1)\n",
       "        (fc): Sequential(\n",
       "          (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (1): PReLU(num_parameters=1)\n",
       "          (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (3): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "      (conv): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
       "      (relu): ReLU()\n",
       "      (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (d4): DownSample2x(\n",
       "      (0): Conv1d(64, 64, kernel_size=(2,), stride=(2,))\n",
       "      (1): ReLU(inplace=True)\n",
       "    )\n",
       "    (c4): ResConv1d(\n",
       "      (cal): Sequential(\n",
       "        (0): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "        (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "        (3): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "        (4): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (se): SELayer(\n",
       "        (avg_pool): AdaptiveAvgPool1d(output_size=1)\n",
       "        (fc): Sequential(\n",
       "          (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (1): PReLU(num_parameters=1)\n",
       "          (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (3): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "      (conv): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
       "      (relu): ReLU()\n",
       "      (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (d5): DownSample2x(\n",
       "      (0): Conv1d(64, 64, kernel_size=(2,), stride=(2,))\n",
       "      (1): ReLU(inplace=True)\n",
       "    )\n",
       "    (c5): ResConv1d(\n",
       "      (cal): Sequential(\n",
       "        (0): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "        (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "        (3): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "        (4): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (se): SELayer(\n",
       "        (avg_pool): AdaptiveAvgPool1d(output_size=1)\n",
       "        (fc): Sequential(\n",
       "          (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (1): PReLU(num_parameters=1)\n",
       "          (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (3): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "      (conv): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
       "      (relu): ReLU()\n",
       "      (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (d6): DownSample2x(\n",
       "      (0): Conv1d(64, 64, kernel_size=(2,), stride=(2,))\n",
       "      (1): ReLU(inplace=True)\n",
       "    )\n",
       "    (c6): ResConv1d(\n",
       "      (cal): Sequential(\n",
       "        (0): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "        (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "        (3): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "        (4): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (se): SELayer(\n",
       "        (avg_pool): AdaptiveAvgPool1d(output_size=1)\n",
       "        (fc): Sequential(\n",
       "          (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (1): PReLU(num_parameters=1)\n",
       "          (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (3): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "      (conv): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
       "      (relu): ReLU()\n",
       "      (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "    (cls): Linear(in_features=2560, out_features=3, bias=True)\n",
       "  ),\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class F1Loss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(F1Loss, self).__init__()\n",
    "\n",
    "    def forward(self, input, target, eps=1e-10):\n",
    "        loss = 0\n",
    "        for idx, i in enumerate(torch.eye(3).cuda()):\n",
    "            t = i.view(3,1)\n",
    "            y_pred_ = input.matmul(t).squeeze()\n",
    "            y_true_ = target==idx\n",
    "            loss += 0.5 * (y_true_ * y_pred_).sum() / (y_true_ + y_pred_ + eps).sum()\n",
    "        return -torch.log(loss+eps)\n",
    "    \n",
    "net2 = NeuralNetClassifier(\n",
    "    MyModule,\n",
    "    max_epochs=20,\n",
    "    lr=0.0001,\n",
    "    batch_size=256,\n",
    "    optimizer=adamod.AdaMod,\n",
    "    iterator_train__shuffle=True,\n",
    "    criterion=F1Loss,\n",
    "    train_split=predefined_split(valid_ds),\n",
    "    callbacks=[EpochScoring(macrof1, use_caching=True, lower_is_better=False),\n",
    "               EpochScoring(microf1, use_caching=True, lower_is_better=False),\n",
    "               Checkpoint(monitor='macrof1_best', dirname='02exp_f1_')],\n",
    "    device='cuda'\n",
    ")\n",
    "\n",
    "net2.initialize() \n",
    "net2.load_params(f_params='02exp/params.pt')\n",
    "net2.partial_fit(tra_features,tra_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
